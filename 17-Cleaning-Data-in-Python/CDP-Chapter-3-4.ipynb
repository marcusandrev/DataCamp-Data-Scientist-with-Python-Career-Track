{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c9536",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c5a2",
   "metadata": {},
   "source": [
    "### Course Description\n",
    "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions. Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d5236",
   "metadata": {},
   "source": [
    "## 3. Advanced data problems\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c751f05",
   "metadata": {},
   "source": [
    "### Uniform currencies\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame. The dataset contains data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. The pandas package has been imported as pd, and the banking DataFrame is in your environment.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.\n",
    "- Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.\n",
    "- Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1b4b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cust_id</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>Age</th>\n",
       "      <th>acct_amount</th>\n",
       "      <th>inv_amount</th>\n",
       "      <th>fund_A</th>\n",
       "      <th>fund_B</th>\n",
       "      <th>fund_C</th>\n",
       "      <th>fund_D</th>\n",
       "      <th>account_opened</th>\n",
       "      <th>last_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>870A9281</td>\n",
       "      <td>1962-06-09</td>\n",
       "      <td>58</td>\n",
       "      <td>63523.31</td>\n",
       "      <td>51295</td>\n",
       "      <td>30105.0</td>\n",
       "      <td>4138.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>15632.0</td>\n",
       "      <td>02-09-18</td>\n",
       "      <td>22-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>166B05B0</td>\n",
       "      <td>1962-12-16</td>\n",
       "      <td>58</td>\n",
       "      <td>38175.46</td>\n",
       "      <td>15050</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>28-02-19</td>\n",
       "      <td>31-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BFC13E88</td>\n",
       "      <td>1990-09-12</td>\n",
       "      <td>34</td>\n",
       "      <td>59863.77</td>\n",
       "      <td>24567</td>\n",
       "      <td>10323.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>25-04-18</td>\n",
       "      <td>02-04-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F2158F66</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>35</td>\n",
       "      <td>84132.10</td>\n",
       "      <td>23712</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>07-11-17</td>\n",
       "      <td>08-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7A73F334</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>30</td>\n",
       "      <td>120512.00</td>\n",
       "      <td>93230</td>\n",
       "      <td>12158.4</td>\n",
       "      <td>51281.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>18383.0</td>\n",
       "      <td>14-05-18</td>\n",
       "      <td>19-07-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   cust_id  birth_date  Age  acct_amount  inv_amount   fund_A  \\\n",
       "0           0  870A9281  1962-06-09   58     63523.31       51295  30105.0   \n",
       "1           1  166B05B0  1962-12-16   58     38175.46       15050   4995.0   \n",
       "2           2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   \n",
       "3           3  F2158F66  1985-11-03   35     84132.10       23712   3908.0   \n",
       "4           4  7A73F334  1990-05-17   30    120512.00       93230  12158.4   \n",
       "\n",
       "    fund_B   fund_C   fund_D account_opened last_transaction  \n",
       "0   4138.0   1420.0  15632.0       02-09-18         22-02-19  \n",
       "1    938.0   6696.0   2421.0       28-02-19         31-10-18  \n",
       "2   4590.0   8469.0   1185.0       25-04-18         02-04-18  \n",
       "3    492.0   6482.0  12830.0       07-11-17         08-11-18  \n",
       "4  51281.0  13434.0  18383.0       14-05-18         19-07-18  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "banking = pd.read_csv('datasets/banking.csv')\n",
    "banking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "# lack of data in the given datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2ae82",
   "metadata": {},
   "source": [
    "### Uniform dates\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The banking DataFrame is in your environment and pandas was imported as pd.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Print the header of account_opened from the banking DataFrame and take a look at the different results.\n",
    "\n",
    "\n",
    "- Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.\n",
    "\n",
    "\n",
    "- Extract the year from the amended account_opened column and assign it to the acct_year column.\n",
    "- Print the newly created acct_year column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01faf9d5",
   "metadata": {},
   "source": [
    "### How's our data integrity?\n",
    "New data has been merged into the banking DataFrame that contains details on how investments in the inv_amount column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the age and birth_date columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' birthdays. Both pandas and datetime have been imported as pd and dt respectively.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
    "- Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
    "\n",
    "\n",
    "- Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n",
    "- Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ed535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv =  banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bde693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c0ab9",
   "metadata": {},
   "source": [
    "### Missing investors\n",
    "Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\n",
    "\n",
    "You just received a new version of the banking DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing inv_amount values.\n",
    "\n",
    "You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The pandas, missingno and matplotlib.pyplot packages have been imported as pd, msno and plt respectively. The banking DataFrame is in your environment.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Print the number of missing values by column in the banking DataFrame.\n",
    "- Plot and show the missingness matrix of banking with the msno.matrix() function.\n",
    "\n",
    "\n",
    "- Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors.\n",
    "\n",
    "\n",
    "- Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc900d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking['inv_amount'].isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb859aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values('age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a331b",
   "metadata": {},
   "source": [
    "### Follow the money\n",
    "In this exercise, you're working with another version of the banking DataFrame that contains missing values for both the cust_id column and the acct_amount column.\n",
    "\n",
    "You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing cust_id don't really help you, and that on average acct_amount is usually 5 times the amount of inv_amount.\n",
    "\n",
    "In this exercise, you will drop rows of banking with missing cust_ids, and impute missing values of acct_amount with some domain knowledge.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.\n",
    "- Use inv_amount to compute the estimated account amounts for banking_fullid by setting the amounts equal to inv_amount * 5, and assign the results to acct_imp.\n",
    "- Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount']*5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40060b23",
   "metadata": {},
   "source": [
    "## 4. Record linkage\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adada0e6",
   "metadata": {},
   "source": [
    "### The cutoff point\n",
    "In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the cuisine_type column is riddled with typos, and should contain only italian, american and asian cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the fuzzywuzzy's process.extract() function by finding the similarity score of the most distant typo of each category.\n",
    "\n",
    "<b>Instructions</b>\n",
    "- Import process from fuzzywuzzy.\n",
    "- Store the unique cuisine_types into unique_types.\n",
    "- Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30beabf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>addr</th>\n",
       "      <th>city</th>\n",
       "      <th>phone</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kokomo</td>\n",
       "      <td>6333 w. third st.</td>\n",
       "      <td>la</td>\n",
       "      <td>2139330773</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feenix</td>\n",
       "      <td>8358 sunset blvd. west</td>\n",
       "      <td>hollywood</td>\n",
       "      <td>2138486677</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>parkway</td>\n",
       "      <td>510 s. arroyo pkwy .</td>\n",
       "      <td>pasadena</td>\n",
       "      <td>8187951001</td>\n",
       "      <td>californian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>r-23</td>\n",
       "      <td>923 e. third st.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>2136877178</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gumbo</td>\n",
       "      <td>6333 w. third st.</td>\n",
       "      <td>la</td>\n",
       "      <td>2139330358</td>\n",
       "      <td>cajun/creole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     name                      addr         city       phone  \\\n",
       "0           0   kokomo         6333 w. third st.           la  2139330773   \n",
       "1           1   feenix   8358 sunset blvd. west     hollywood  2138486677   \n",
       "2           2  parkway      510 s. arroyo pkwy .     pasadena  8187951001   \n",
       "3           3     r-23          923 e. third st.  los angeles  2136877178   \n",
       "4           4    gumbo         6333 w. third st.           la  2139330358   \n",
       "\n",
       "           type  \n",
       "0      american  \n",
       "1      american  \n",
       "2   californian  \n",
       "3      japanese  \n",
       "4  cajun/creole  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "restaurants = pd.read_csv('datasets/restaurants_L2_dirty.csv')\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112935ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('indonesian', 72), ('italian', 67), ('russian', 67), ('american', 62), ('californian', 54), ('japanese', 54), ('mexican/tex-mex', 54), ('american ( new )', 54), ('mexican', 50), ('cajun/creole', 36), ('middle eastern', 36), ('vietnamese', 36), ('pacific new wave', 36), ('fast food', 36), ('chicken', 33), ('hamburgers', 27), ('hot dogs', 26), ('coffeebar', 26), ('continental', 26), ('steakhouses', 25), ('southern/soul', 22), ('delis', 20), ('eclectic', 20), ('pizza', 20), ('health food', 19), ('diners', 18), ('coffee shops', 18), ('noodle shops', 18), ('french ( new )', 18), ('desserts', 18), ('seafood', 17), ('chinese', 17)]\n",
      "[('american', 100), ('american ( new )', 90), ('mexican', 80), ('mexican/tex-mex', 68), ('asian', 62), ('italian', 53), ('russian', 53), ('middle eastern', 51), ('pacific new wave', 45), ('hamburgers', 44), ('indonesian', 44), ('chicken', 40), ('southern/soul', 39), ('japanese', 38), ('eclectic', 38), ('delis', 36), ('pizza', 36), ('cajun/creole', 34), ('french ( new )', 34), ('vietnamese', 33), ('californian', 32), ('diners', 29), ('desserts', 25), ('coffeebar', 24), ('steakhouses', 21), ('seafood', 13), ('chinese', 13), ('fast food', 12), ('coffee shops', 11), ('noodle shops', 11), ('health food', 11), ('continental', 11), ('hot dogs', 0)]\n",
      "[('italian', 100), ('asian', 67), ('californian', 56), ('continental', 51), ('indonesian', 47), ('russian', 43), ('mexican', 43), ('american', 40), ('japanese', 40), ('mexican/tex-mex', 39), ('american ( new )', 39), ('pacific new wave', 39), ('vietnamese', 35), ('delis', 33), ('pizza', 33), ('diners', 31), ('middle eastern', 30), ('chicken', 29), ('chinese', 29), ('health food', 27), ('southern/soul', 27), ('cajun/creole', 26), ('steakhouses', 26), ('seafood', 14), ('hot dogs', 13), ('noodle shops', 13), ('eclectic', 13), ('french ( new )', 13), ('desserts', 13), ('hamburgers', 12), ('fast food', 12), ('coffeebar', 12), ('coffee shops', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de730d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
